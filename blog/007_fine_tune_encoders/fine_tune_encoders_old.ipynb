{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Fine-Tuning Embedding Models with Unsloth\n",
    "author: Chris Kroenke\n",
    "draft: false\n",
    "date: '2025-03-21'\n",
    "date-modified: '2025-03-21'\n",
    "image: modernBERT_title_image.png\n",
    "toc: true\n",
    "description: Fine-tuning a BERT embeddings model with QLoRA using unsloth and Sentence Transformers.\n",
    "tags:\n",
    "  - embeddings\n",
    "  - fine-tuning\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "    grid:\n",
    "      body-width: 1200px\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-word !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We are finally at the point that many people have been waiting for: small LLMs have become very powerful and can run on consumer GPUs. With good fine-tuning in a given domain, they even rival some of the best commercially available LLMs.\n",
    "\n",
    "The combo of being runnable and fine-tuneable on consumer hardware is possible thanks to weight quantization and LoRA adapters, respectively. \n",
    "\n",
    "This post fine-tunes a text embedding model with the unsloth and Sentence Transformers libraries. Specifically, we fine-tune a set of QLoRA adapters using a contrastive loss on a simple Question and Answer dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `unsloth` library\n",
    "\n",
    "The [`unsloth`](https://unsloth.ai/) library makes it both efficient and affordable to fine-tune transformer networks on consumer hardware.\n",
    "\n",
    "Most of their work focuses on fine-tuning decoder models, aka the LLM family of models. This makes sense given the high visibility and ever-increasing capabilities of generative networks.\n",
    "\n",
    "Unsloth has an ocean of starter notebooks that make it easy for anyone to fine-tune relevant, modern LLMs. Many of the notebooks use quantization setups that even fit on 8GB GPUs. If you went back a few years ago, and told people we'd be able to meaningfully fine-tune powerful, SoTA LLMs on such small cards it would have sounded outlandish.\n",
    "\n",
    "While it makes a ton of sense that generative LLMs receive so much attention, there is also the other side of the architecture coin: encoder models. These are models like BERT that transform sentences into vector embeddings that capture its semantic content and relationships. \n",
    "\n",
    "Encoder models power incredibly useful tools like RAG. Despite the LLM hype, it is RAG engines that are the backbone of most LLM applications currently deployed in the wild. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG workhorses\n",
    "\n",
    "RAG engines rely on text embedding models, aka the encoder side of transformer networks.  \n",
    "\n",
    "There is a great post here from the creators of the recent [`modernBERT`](https://huggingface.co/blog/modernbert) embedding model that describes how LLMs capture all the hype and fanfare, but encoding models are the actual workhorses for AI products.\n",
    "\n",
    "Unfortunately, as of writing, unsloth does not support fine-tuning encoder models. It's been a feature in their pipeline for a while, but they understandably have a ton of other pressing work. \n",
    "\n",
    "We can still however leverage some recent PRs, along with the Sentence Transformers library, to patch fine-tuning embeddings into unsloth. \n",
    "\n",
    "In this post, we will fine-tune an `all-MiniLM` model, specifically the recent [all-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning embeddings with Unsloth\n",
    "\n",
    "Here is the rough process we'll go through. We will the all-MiniLM model and wrap it with unsloth's QLoRA adapters. Then, we'll again wrap the unsloth-patched model inside of a custom Sentence Transformers model. It is this final double-wrapped model that will be fine-tuned. \n",
    "\n",
    "Both Sentence Transformers and unsloth actually subclass HuggingFace's Trainer and TrainingArguments. Their APIs and functionality are not quite identical, but are close enough for our purposes. \n",
    "\n",
    "Sentence Transformers will do the heavy lifting of the learning loop: preparing the input batches, computing the embeddings-specific loss, and handling the weight updates.\n",
    "\n",
    "Let's get started and put all of this together. First, we need to prepare our environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Unsloth \n",
    "\n",
    "The following command installs unsloth:  \n",
    "\n",
    "```bash\n",
    "pip install unsloth\n",
    "```  \n",
    "\n",
    "But unsloth is under constant development. It directly patches and modifies many low-level libraries used for LLM inference and training. Because of this, it can be quite tricky to install. The default setup in their Google Colab notebooks do a specific pip installation dance that is quite robust. \n",
    "\n",
    "> You may have good luck with the simple `pip install unsloth`. Depending on your linux setup, it might not be so simple. If the simple install fails, mimic the Colab-specific pip installation below.  \n",
    "\n",
    "```bash\n",
    "# only do this if the simple pip install fails\n",
    "pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "pip install --no-deps unsloth\n",
    "```\n",
    "\n",
    "Once this is ready, we import unsloth and get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unsloth first so it can patch in optimizations\n",
    "import unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it's best-practice to first import unsloth. This lets is patch all the lower-level libraries it needs. Then we'll be using the `FastModel` class. This takes a very handy `auto_class` argument that lets us load encoder models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads encoder models\n",
    "from unsloth import FastModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can bring in all of our regular imports. We'll import them all here, then mention them when it's needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some setup\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Import the correct base model class for our model\n",
    "from transformers import BertModel\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from peft import LoraConfig, TaskType\n",
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformerTrainingArguments, SentenceTransformerTrainer, SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start defining the variables we'll need. Let's start by setting the all-MiniLM model and its class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "BASE_MODEL_ID = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "BERT_MODEL = BertModel\n",
    "\n",
    "# Maximum sequence length of this model\n",
    "MAX_SEQ_LENGTH = 512\n",
    "LOAD_IN_4BIT = True  # For QLoRA (4-bit quantization) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load this `FastModel`. This is the full model, before we've attached QLoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = BASE_MODEL_ID,\n",
    "    auto_model = BERT_MODEL,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = None, # Auto-detects (BF16/FP16)\n",
    "    load_in_4bit = LOAD_IN_4BIT,\n",
    ")\n",
    "print(f\"Loaded {BASE_MODEL_ID} with Unsloth.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRA patches\n",
    "\n",
    "Next, we'll patch in the QLoRA weights to be learned using the unsloth library. Unsloth has a whole set of good default arguments that have been earned and hard-won for fine-tuning LLMs. From my initial experiments, it seems like some of these will need re-thinking for encoder models. But, they are certainly a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LORA Configuration\n",
    "LORA_R = 16          # Rank of the LORA matrices. \n",
    "LORA_ALPHA = 32      \n",
    "LORA_DROPOUT = 0.0   # Dropout of 0 is best.\n",
    "USE_RSLORA = False   # Rank-Stabilized LoRA if desired\n",
    "\n",
    "# Target modules for adapters\n",
    "LORA_TARGET_MODULES = [\"query\", \"key\", \"value\", \"dense\"] \n",
    "LORA_EXCLUDE_MODULES = [] # put anything you want to skip here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our QLoRA parameters we can attach them to our base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attaching QLoRA adapters...\")\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_R,\n",
    "    lora_alpha = LORA_ALPHA,\n",
    "    lora_dropout = LORA_DROPOUT,\n",
    "    target_modules = LORA_TARGET_MODULES,\n",
    "    exclude_modules = LORA_EXCLUDE_MODULES,\n",
    "    use_rslora = USE_RSLORA,\n",
    "    bias = \"none\", # Standard practice for LoRA\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    modules_to_save = None, # Add to train non-LORA modules\n",
    "    task_type = TaskType.FEATURE_EXTRACTION, # Important!\n",
    ")\n",
    "print(\"LORA adapters added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key part here is the line `task_type = TaskType.FEATURE_EXTRACTION` which sets the models up for embedding tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how QLoRA only learn a fraction of the model's original parameters, making it feasible to run this training on powerful consumer hardware instead of massive clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many parameters we will actually learn\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping unsloth model with Sentence Transformers\n",
    "\n",
    "To use an embeddings-specific loss, we need the Sentence Transformers library. We basically need to patch in our own model into a regular ST setup. \n",
    "\n",
    "Once we have the QLoRA-patched embeddings model, we can follow the Sentence Transformers documentation to create a custom model. There are only a few set of requirements we need.\n",
    "  \n",
    "We need to manually create a Transformer model. For this, we'll directly pass in our embeddings model.\n",
    "\n",
    "We also need to tell Sentence Transformers how to convert the models' final output into an embedding. This is called the pooling stage. There are a ton of techniques, but it seems like mean-pooling is currently winning out. Mean pooling means we basically take the token-wise average of the network's final activations and call that final single vector the embedding. \n",
    "  \n",
    "Lastly, many models include a normalization stage. This determines whether or not we scale vectors to have a uniform unit length. It's the default for sentence transformers, and in practice I've found it's saved me a lot of headache to always and only deal with normalized vectors. \n",
    "\n",
    "Next, we pass our three modules into a SentenceTransformer class, which create the final model that can be used by the library's Trainer class.\n",
    "\n",
    "> Note: you can also pass in additional arguments here that would have typically be passed to the huggingface model, such as the attention implementation.\n",
    "\n",
    "Phew. That's a lot. Let's write a function to make our life a bit easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the ST model\n",
    "def get_st_unsloth_wrapper(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        base_model_id=BASE_MODEL_ID,\n",
    "        pooling_mode=\"mean\",\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        ):\n",
    "    print(\"Initializing Sentence Transformer modules...\")\n",
    "\n",
    "    # 1. Create the Transformer module instance\n",
    "    transformer_module = sentence_transformers.models.Transformer(\n",
    "        model_name_or_path=base_model_id,\n",
    "        max_seq_length=max_seq_length,\n",
    "    )\n",
    "\n",
    "    # 2. Replace the internal Hugging Face model with our LORA-patched Unsloth model\n",
    "    transformer_module.auto_model = model\n",
    "    transformer_module.tokenizer = tokenizer\n",
    "\n",
    "    print(f\"Manually assigned Unsloth LORA model to sentence_transformers.models.Transformer module.\")\n",
    "\n",
    "    # 3. Create the Pooling module\n",
    "    hidden_size = model.config.hidden_size\n",
    "    pooling_module = sentence_transformers.models.Pooling(\n",
    "        word_embedding_dimension=hidden_size,\n",
    "        pooling_mode=pooling_mode,\n",
    "    )\n",
    "    print(f\"Using Pooling module with mode: {pooling_mode}\")\n",
    "\n",
    "    # 4. Add the Normalize module\n",
    "    normalize_module = sentence_transformers.models.Normalize()\n",
    "    modules = [transformer_module, pooling_module, normalize_module]\n",
    "\n",
    "    # 5. Initialize SentenceTransformer with custom modules\n",
    "    sbert_model = SentenceTransformer(modules=modules)\n",
    "\n",
    "    print(f\"SentenceTransformer wrapper created with custom modules.\")\n",
    "    return sbert_model\n",
    "\n",
    "# wrap our unsloth model in Sentence Transformers\n",
    "sbert_model = get_st_unsloth_wrapper(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        base_model_id=BASE_MODEL_ID,\n",
    "        pooling_mode=\"mean\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Now that the model is setup, let's get the data ready. This is the data setup from Philip Schmidd's notebook, grabbed here since we're mainly focused on the unsloth and QLoRA setup.\n",
    "\n",
    "The main thing we need to do is properly format this dataset for the contrastive loss we will be using.\n",
    "\n",
    "A proper deep dive into contrastive losses is far beyond the scope of this guide. Here's an excellent reference that teaches you all the basics (and then some) you'll likely need to go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"philschmid/finanical-rag-embedding-dataset\", split=\"train\")\n",
    "\n",
    "# rename columns\n",
    "dataset = dataset.rename_column(\"question\", \"anchor\")\n",
    "dataset = dataset.rename_column(\"context\", \"positive\")\n",
    "\n",
    "# Add an id column to the dataset\n",
    "dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "\n",
    "# split dataset into a 10% test set\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key takeaway is that all the hard research into contrastive losses has paid of tremendously: it has resulted in a certain kind of loss, called MBCE, that makes it possible to train embeddings model with loosely, implicitly labeled data like Q&A pairs. \n",
    "\n",
    "Question and Answer pairs became a pair of reference (anchor) and matching (positive) vectors that should cluster together.\n",
    "\n",
    "During training, the model randomly picking matching vectors from *different* training example in the same batch to use as a negative. \n",
    "\n",
    "This means all you need to start training a good embeddings model is a good set of Q&A questions.\n",
    "\n",
    "With how ubiquitous and powerful this kind of data has become thanks to SFT and reasoning-based RL, you can see how we're very close to an insanely powerful data bootstrapping feedback loop. It's just around the corner...\n",
    "\n",
    "And, we can always do some more work to improve this loss, and pick better negative examples. But as an aside, it is pretty outrageous and lucky how quickly we can set up fine-tuning embeddings models with this loss.\n",
    "\n",
    "Let's go ahead and define this powerful loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "loss = MultipleNegativesRankingLoss(sbert_model) \n",
    "print(f\"Using loss: {type(loss).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define and group up all of our training arguments. A rule of thumb is that LoRA can overfit if it's trained for too many epochs. So we start with one, but this is definitely a parameter to play with.\n",
    "\n",
    "For our batch size, you should use the largest one that fits in your GPU's VRAM. This is especially important for our loss, since it randomly picks other negative examples from the same batch. The larger the batch size, the more random negative examples it can pick from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing all of our training arguments \n",
    "\n",
    "# Training Configuration \n",
    "NUM_TRAIN_EPOCHS = 1              # Start with 1 epochs\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 512   # Adjust based on GPU VRAM and MAX_SEQ_LENGTH.\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 1024   # Can usually be higher than train batch size.\n",
    "GRADIENT_ACCUMULATION_STEPS = 1   # Only for small cards\n",
    "\n",
    "# don't repeat samples in the same batch given our loss\n",
    "batch_sampler = BatchSamplers.NO_DUPLICATES if isinstance(loss, MultipleNegativesRankingLoss) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the training arguments are standard for unsloth models. However, QLoRA for encoders is a quite unexplored space. There are likely far more optimal configurations, but this is a good start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set lower for longer training runs\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "WARMUP_RATIO = 0.1                 # Percent of warmup steps\n",
    "OPTIMIZER = \"adamw_8bit\"           # start with 8bit optimizer\n",
    "LR_SCHEDULER_TYPE = \"cosine\"       # schedule for the lr\n",
    "WEIGHT_DECAY = 0.1                 # Weight decay \n",
    "FP16 = not torch.cuda.is_bf16_supported() # Use FP16 if BF16 is not available\n",
    "BF16 = torch.cuda.is_bf16_supported()     # Use BF16 on supported GPUs (Ampere+) for stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define how we'll evaluate the model. We'll also define where to save the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the output directory\n",
    "OUTPUT_DIR = Path(\"finetuned_embeddings\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# evaluation and saving\n",
    "EVAL_STRATEGY = \"steps\"\n",
    "EVAL_STEPS = 8        # evaluate every N steps\n",
    "SAVE_STRATEGY = \"steps\"\n",
    "SAVE_STEPS = 8       # save checkpoint every N steps\n",
    "SAVE_TOTAL_LIMIT = 2    # keep only the last N checkpoints\n",
    "LOGGING_STEPS = 10      # log metrics every N steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the specific evaluation setup is borrowed from Philip Schmidd's notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "train_dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
    "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "\n",
    "# Convert the datasets to dictionaries\n",
    "corpus = dict(\n",
    "    zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])\n",
    ")  # Our corpus (cid => document)\n",
    "queries = dict(\n",
    "    zip(test_dataset[\"id\"], test_dataset[\"anchor\"])\n",
    ")  # Our queries (qid => question)\n",
    "\n",
    "# Create a mapping of relevant document (1 in our case) for each query\n",
    "relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
    "for q_id in queries:\n",
    "    relevant_docs[q_id] = [q_id]\n",
    "\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(\n",
    "    queries=queries,\n",
    "    corpus=corpus,\n",
    "    relevant_docs=relevant_docs,\n",
    "    score_functions={\"cosine\": cos_sim},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining training arguments...\")\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Core Training Parameters\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    optim=OPTIMIZER,\n",
    "    batch_sampler=batch_sampler,\n",
    "    fp16=FP16,\n",
    "    bf16=BF16,\n",
    "    tf32=True, # NOTE: gpu must support\n",
    "    fp16_full_eval=True,\n",
    "    # Evaluation and Saving\n",
    "    eval_strategy=EVAL_STRATEGY,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "    load_best_model_at_end=True if evaluator else False,\n",
    "    metric_for_best_model=\"eval_sts-eval_spearman_cosine\" if evaluator and isinstance(evaluator, InformationRetrievalEvaluator) else None,\n",
    "    greater_is_better=True, \n",
    "    # Logging and Reporting\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    report_to=\"tensorboard\",\n",
    "    run_name=f\"{BASE_MODEL_ID.split('/')[-1]}-st-finetune\",\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing SentenceTransformerTrainer...\")\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=sbert_model, # Pass the standard SentenceTransformer model\n",
    "    args=args,\n",
    "    train_dataset=train_dataset.select_columns([\"anchor\", \"positive\"]),\n",
    "    eval_dataset=test_dataset.select_columns([\"anchor\", \"positive\", \"score\"]) if evaluator else None,\n",
    "    loss=loss,\n",
    "    evaluator=evaluator,\n",
    "    callbacks=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_model = SentenceTransformer(BASE_MODEL_ID)\n",
    "original_model.eval()\n",
    "\n",
    "fine_tuned_model = FastModel.from_pretrained(\n",
    "    args.output_dir, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "fine_tuned_model.eval()\n",
    "fine_tuned_sbert = get_st_unsloth_wrapper(\n",
    "    fine_tuned_model,\n",
    "    tokenizer,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    base_model_id=BASE_MODEL_ID,\n",
    "    pooling_mode=\"mean\",\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "baselines = evaluator(original_model)\n",
    "fine_tuned_results = evaluator(fine_tuned_sbert)\n",
    "\n",
    "# Print the main score\n",
    "print(f\"Original model: {baselines}\")\n",
    "print(f\"Fine-tuned model: {fine_tuned_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
