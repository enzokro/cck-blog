{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Late Chunking on long documents\n",
    "author: Chris Kroenke\n",
    "draft: false\n",
    "date: '2025-04-10'\n",
    "date-modified: '2025-04-10'\n",
    "image: LateChunking_title_image.png\n",
    "toc: true\n",
    "description: Late-chunking might be the closest we've ever been to solving retrieval with text embeddings models. This post gets it working on very long documents.\n",
    "tags:\n",
    "  - embeddings\n",
    "  - retrieval\n",
    "  - RAG\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "    grid:\n",
    "      body-width: 1200px\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-word !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Late Chunking might be the closest we've ever been to solving retrieval with text embeddings models. This post goes over a quick history of RAG in the context of Late Chunking. Then, we see how to apply Late Chunking with a modern encoder model to even extremely long documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recapping RAG\n",
    "\n",
    "It's not an exaggeration to say that Retrieval Augmented Generation (RAG) has been the main enabler of serious production LLM applications. Specifically, it's the ability of RAG to inject precise, relevant information into the conversation that the LLM has not explicitly seen before. RAG is a way to overcome the LLM's frozen pre-training and fine-tuning limitations. Without RAG we would drowning in hallucinations, and no domain that deals with delicate and sensitive data would trust LLMs in production. RAG is not perfect, but it's certainly quite powerful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Challenges of RAG\n",
    "\n",
    "The main issue that RAG faces is finding the relevant content. This is handled by text embedding models, which are able to map the user's query to other external documents and find the ones that are most relevant. The second challenge of RAG is in knowing which relevant results to include. Feeding in all of the documents, relevant though they might be totality, defeats the purpose and one would instead be better off Continuously Pre-Training (CPT) on the documents directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk, embed, then retrieve\n",
    "\n",
    "Both the search and inclusion of relevant context is usually handled by \"chunking\" the external documents. This means breaking up a document into individual, isolated chunks. Each chunk is then embedded and only a fixed number (top-k) is returned to the model. Both the chunk sizes and the number of returned search results are parameters that must be tuned and calibrated per-application and domain for an ideal performance. When faced with a large volume of external data, this chunk -> embed -> retrieve pipeline is the simplest way of making sure only the most relevant parts for the user's current query are shown to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Chunking strategies\n",
    "\n",
    "There are a host of strategies for breaking long documents into more manageable chunks. The most popular approaches are:  \n",
    "\n",
    "- Fixed-size chunks: Each chunk is made up of a fixed number of words, sentences, or tokens. There may also be overlap between the chunks, to partially deal with awkward sentence or context breaks.  \n",
    "- Recursive chunks: Text is initially split into sentences and paragraphs, which are then recursively broken down until they meet the target chunk size. This aims to keep relevant paragraphs and sentences together. \n",
    "- Semantic chunks: Using some measure of intra-document similarity, this chunking strategy tries to keep related sections together to make each chunk extra meaningful.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problems with the chunking of old  \n",
    "\n",
    "The chunking approaches above all share a fundamental issue: each chunk is completely isolated from the rest of the document. The first chunks of a document have absolutely no relation to the last chunks in the document. Worse: neighboring but non-overlapping chunks have no relationship *even to each other*. The text embedding model sees each chunk in isolation, and has no memory of what came before or goes after. \n",
    "\n",
    "There have been a mountain of techniques built upon RAG to handle this issue. Most of these techniques end up being systems built as engineering solutions. But this huge problem is actually caused upstream by the model itself. Interest and research in encoder embedding models has lagged that of their generative decoder LLM counterparts. While there are several commercial and open-source LLMs now with context windows that fit tens of thousands and even a million tokens, for a long time embeddings models have made due with a measly 512 tokens. Taking a step back: it speaks to the power of RAG that so many powerful and useful applications have been built on the back of a fundamentally limited context model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Encoder models\n",
    "\n",
    "Encoder models and the RAG engines built on top of them have been the silent workhorses of serious LLM applications for a while. The interest and research in the models themselves has been picking up. We can see this most clearly in the recent modernBERT models, which applied and leveraged years of decoder-focused LLM research and model enhancements to the ancient (2018) BERT architecture. It reminds me of how vision and NLP used to trade approaches and advances from each other to keep improving. For a long time vision dominated, then NLP caught up and the rest is history.  \n",
    "\n",
    "One of the most significant improvements of modern encoder networks has been an increase in the model's context window. Now there are several encoder models with context windows of 8k tokens. This is already an order of magnitude improvement over the old token limits of 512, but I hope to still look back on even 8k as quaint in a few years. \n",
    "\n",
    "## Long context means new possibilities\n",
    "\n",
    "This new, larger context window unlocked a fundamentally new approach to embedding documents for retrieval workflows. An 8k context window means that roughly 6,000 can fit as input, which is already longer than many kinds of documents. This means we don't have to chunk the document before encoding! We can encode the whole thing at once. \n",
    "\n",
    "# Enter Late Chunking\n",
    "\n",
    "Late Chunking refers to the recent approach of encoding an entire document at once, without first breaking it down into smaller chunks of text. \n",
    "\n",
    "Let's think about what it means to encode an entire document at once, and why it's so different and more powerful than isolated chunking. Encoder models are just like LLMs in that they are built out of Self-Attention Transformer modules. The attention modules are able to attend over and process an element in its input in relation to any other element, including itself. When we encode an entire document at once, that means that we create a *global* embedding context, where even the first sentence has influenced the last ones and everything in between. This directly solves the isolated-context problem of having to chunk texts before embedding, with older models and their 512 context lengths that simply can't fit the longer documents. \n",
    "\n",
    "## Keeping Late Chunking Manageable \n",
    "\n",
    "Assume we've encoded a document that maxes out a new encoder model's 8k context window. This means we end up with one embedding per token, so we have 8k embedding tokens. But we don't want to keep each one as a potential candidate for a RAG search: matching a user's search query to a lone token, regardless of the global context it might have, doesn't make a lot of sense. Instead, we need to group up the vectors belonging to similar or related tokens. \n",
    "\n",
    "This is a loose parallel to the original RAG problem, which has to chunk texts to keep them manageable. Now, we face the more theoretical problem of how to group a string of global-context vectors into related, manageable sub-units. This is a fresh area of research, but the two most direct and promising approaches are: \n",
    "\n",
    "- Fixed token chunks: Break down the 8k global tokens into groups of fixed tokens, usually 256 or 512. This can again be done with or without overlap. And, because context lengths tend to be powers of 2, that means we set a fixed cap on the potential number of vectors from any one document.  \n",
    "- Sentence chunks: Find the tokens that maps back to complete sentences in the original text. Then group up a fixed number of sentences, however many tokens that might be. 5 seems to be a sweet spot for sentences, which loosely ties back to the idea of paragraph chunking.  \n",
    "\n",
    "In initial experiments, these token-gathering approaches give almost identical results. Given the easy of implementation of the first, we'll stick that one in this notebook. \n",
    "\n",
    "Once we have a way of grouping up related tokens (often called token *spans*), we can then take the average of the token's vectors inside that span to end up with a single embedding vector. We've worked our way back to keeping the number of vectors manageable for RAG engines, while also directly solving the isolation problem by creating vectors with global context from the entire document.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Late Long Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8k tokens is quite large, but there are many important documents such as research papers or financial reports that sit well about 6,000 words in length. How do we handle those? We can mimic the older style of sliding window embeddings by breaking up the long document into as many 8k token chunks as we need to. Then, we can Late Chunk each of these 8k windows, and apply our token-grouping and averaging to end up embeddings for the entire, long document. \n",
    "\n",
    "We can also try some experimental approaches to try and retain the global context of the entire long document in each 8k chunk. This also lets us bootstrap a cheap kind of centroid lookup in the case of extremely long documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Late Chunking with nomic.ai's modernBERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all of this into practice using one of the new, powerful encoder models than can handle 8k tokens. We will use Nomic.ai's `[modernbert-embed-base]`(https://huggingface.co/nomic-ai/modernbert-embed-base) model. As an aside, I've been getting outrageously good results with this model and Late Chunking. It's the first time in my work with LLMs that it feels like I almost don't even need an LLM to process and interpret the RAG results. The returned results are so good, that they basically answer my original question directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load up this model with the Sentence Transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# load up the nomic embeddings model\n",
    "model_id  = \"nomic-ai/modernbert-embed-base\"\n",
    "model     = AutoModel.from_pretrained(model_id).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our Late Long Chunking, we'll load a very long document: Apple's 10K filing for 2024 found [here.](https://s2.q4cdn.com/470004039/files/doc_earnings/2024/q4/filing/10-Q4-2024-As-Filed.pdf) We'll extract the text from the document using the `pypdfium2` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF with 121 pages.\n"
     ]
    }
   ],
   "source": [
    "import pypdfium2 as pdfium\n",
    "\n",
    "# load the pdf \n",
    "pdf = pdfium.PdfDocument(\"Apple-10k-filing-2024.pdf\")\n",
    "# check the number of pages\n",
    "print(f'PDF with {len(pdf)} pages.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many PDF parsing libraries, pypdfium works on a page at a time to keep memory in check. Let's write a quick helper that will get the text from all pages. In an actual application, we'd also want to more intelligently handle tables and figures. But for now, let's focus on the power of Late Chunking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cck/projects/cck-blog/.venv/lib/python3.12/site-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
      "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
     ]
    }
   ],
   "source": [
    "def get_text(page):\n",
    "    \"Gets the entire text from a PDF page\"\n",
    "    textpage = page.get_textpage()\n",
    "    text_all = textpage.get_text_range()\n",
    "    return text_all\n",
    "\n",
    "texts = [get_text(page) for page in pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the first few characters from the first page to see what we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549\n",
      "FORM 10-K\n",
      "(Mark One)\n",
      "☒ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the fiscal year ended September 28, 2024\n",
      "or\n",
      "☐ TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the transition period from              to             .\n",
      "Commission File Number: 001-36743\n",
      "Apple Inc.\n",
      "(Exact name of Registrant as specified in its charter)\n",
      "California 94-24041\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][:512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you look at the raw string there are a ton of special characters and things we want to clean up for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNITED STATES\\r\\nSECURITIES AND EXCHANGE COMMISSION\\r\\nWashington, D.C. 20549\\r\\nFORM 10-K\\r\\n(Mark One)\\r\\n☒ '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put together a short function that will clear up the return characters and non-breaking spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_pdf_text(text: str) -> str:\n",
    "    # Normalize newlines\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\")\n",
    "    # Replace non-breaking spaces\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "    # Collapse multiple paragraph breaks to two newlines\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    # Collapse all whitespace sequences to single spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Strip leading/trailing whitespace\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549 FORM 10-K (Mark One) ☒ ANNUA'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_clean = clean_pdf_text(texts[0])\n",
    "sample_clean[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's go ahead and clean up all of the text from the PDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = [clean_pdf_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count up both the total and word length of these documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "409552"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_chars = sum(len(t) for t in clean_texts); total_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64162"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = sum(len(t.split(' ')) for t in clean_texts); total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite a lot of words. Apple is doing well, one hopes. But models don't see words - they see tokens. How many tokens are we dealing with?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85054"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many total tokens we have\n",
    "all_text = ' '.join(clean_texts)\n",
    "all_tokens = model.tokenizer(all_text);\n",
    "num_total_tokens = len(all_tokens[0]); num_total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 85,054 tokens after some light cleaning of the text. How many windows will we need to fully encode this very long document? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many tokens we can fit in the nomic ai model\n",
    "model_max = model.tokenizer.model_max_length; model_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many non-overlapping windows we need for the long document\n",
    "num_windows = (num_total_tokens // model_max) + 1; num_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for Long Late Chunking\n",
    "\n",
    "Now that we've loaded and cleaned up the text, we can get to the actual Late Chunking algorithm. There are some excellent blog posts from Issac Flath, and papers and articles from the Jina.ai and Weaviate teams that go into the details of Late Chunking. \n",
    "\n",
    "- [Isaac Flath's Late Chunking Blog Post](https://isaacflath.com/blog/blog_post?fpath=posts%2F2025-04-08-LateChunking.ipynb)\n",
    "- [Jina.ai Article on Late Chunking](https://jina.ai/news/late-chunking-in-long-context-embedding-models/)\n",
    "- [Jina.ai and Weaviate Paper](https://arxiv.org/pdf/2409.04701)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is based on the articles and paper above, with some slight modifications to handle our Late Long Chunking. We start by importing everything we need, and choosing a hardware accelerator if it's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# long context window of modern encoders\n",
    "MODEL_MAX_LENGTH = 8192 \n",
    "\n",
    "def _get_device() -> str:\n",
    "    \"\"\"Gets the appropriate device.\"\"\"\n",
    "    if torch.cuda.is_available(): return \"cuda\"\n",
    "    if torch.backends.mps.is_available(): return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "DEVICE = _get_device(); DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a function to do regular Late Chunking on a single window. We'll use it to group the global tokens into fixed-sized groups, with potential overlap, to then create the final embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def late_chunking(\n",
    "    window_text: str,\n",
    "    model: AutoModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    chunk_token_size: int,\n",
    "    chunk_token_overlap: int,\n",
    "    model_processing_max_len: int \n",
    "):\n",
    "    \"\"\"\n",
    "    Performs late chunking on a given text window to create fixed-size token chunks.\n",
    "\n",
    "    The window_text is first embedded entirely by the model. Then, fixed-size token \n",
    "    chunks are derived from these token embeddings by pooling. The corresponding\n",
    "    text for each chunk is also extracted.\n",
    "\n",
    "    Args:\n",
    "        window_text: The text content of the current window (assumed to fit model context).\n",
    "        model: The pre-trained Hugging Face model, moved to the correct device.\n",
    "        tokenizer: The pre-trained Hugging Face tokenizer.\n",
    "        chunk_token_size: The desired size of each final chunk in tokens.\n",
    "        chunk_token_overlap: The number of tokens to overlap between final chunks.\n",
    "        model_processing_max_len: The maximum number of tokens the model should process \n",
    "                                 for this window_text (typically model's max context length).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - A list of chunk text strings.\n",
    "            - A NumPy array of chunk embeddings (L2 normalized). Returns empty if no\n",
    "              chunks are produced.\n",
    "    \"\"\"\n",
    "    if not window_text.strip():\n",
    "        return [], np.array([])\n",
    "\n",
    "    # Tokenize the window text, ensuring truncation and padding for the model\n",
    "    inputs = tokenizer(\n",
    "        window_text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=model_processing_max_len,\n",
    "        padding=\"max_length\", # Pad to model_processing_max_len for consistent tensor shapes\n",
    "        return_offsets_mapping=True # Essential for mapping tokens back to text\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Get token embeddings for the entire window\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != 'offset_mapping'}) \n",
    "    \n",
    "    # Squeeze batch dimension and move to CPU for easier handling\n",
    "    token_embeddings_padded = outputs.last_hidden_state.squeeze(0).cpu() # Shape: (model_processing_max_len, embed_dim)\n",
    "    attention_mask = inputs['attention_mask'].squeeze(0).cpu()           # Shape: (model_processing_max_len)\n",
    "    offsets_padded = inputs['offset_mapping'].squeeze(0).cpu()           # Shape: (model_processing_max_len, 2)\n",
    "\n",
    "    # Determine the actual number of non-padding tokens in this window\n",
    "    actual_num_tokens = attention_mask.sum().item()\n",
    "    print(f'actual_num_tokens', actual_num_tokens)\n",
    "    \n",
    "    if actual_num_tokens == 0: \n",
    "        return [], np.array([]) # No actual content tokens\n",
    "\n",
    "    # Use only the non-padded parts for creating chunks\n",
    "    token_embeddings = token_embeddings_padded[:actual_num_tokens]\n",
    "    offsets = offsets_padded[:actual_num_tokens]\n",
    "\n",
    "    chunk_texts: List[str] = []\n",
    "    chunk_embeddings_list: List[torch.Tensor] = [] # Store individual torch tensors\n",
    "    \n",
    "    stride = chunk_token_size - chunk_token_overlap\n",
    "    if stride <= 0:\n",
    "        raise ValueError(f\"chunk_token_size ({chunk_token_size}) must be greater than \"\n",
    "                         f\"chunk_token_overlap ({chunk_token_overlap}) to make progress.\")\n",
    "\n",
    "    # Iterate through the actual tokens of the window to create fixed-size token chunks\n",
    "    for i in range(0, actual_num_tokens, stride):\n",
    "        start_token_idx = i\n",
    "        end_token_idx = min(i + chunk_token_size, actual_num_tokens)\n",
    "        \n",
    "        # Skip if the current slice would be empty or invalid\n",
    "        if start_token_idx >= end_token_idx:\n",
    "            continue\n",
    "\n",
    "        current_chunk_token_embeddings = token_embeddings[start_token_idx:end_token_idx]\n",
    "        \n",
    "        # This should not happen if start_token_idx < end_token_idx, but as a safe guard\n",
    "        if current_chunk_token_embeddings.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # Mean pool the token embeddings for the current chunk\n",
    "        pooled_embedding = current_chunk_token_embeddings.mean(dim=0)\n",
    "        chunk_embeddings_list.append(pooled_embedding)\n",
    "\n",
    "        # Extract the original text for the current chunk using character offsets\n",
    "        # offsets are 0-indexed; start_token_idx is inclusive, end_token_idx is exclusive\n",
    "        char_start = offsets[start_token_idx, 0].item()\n",
    "        char_end = offsets[end_token_idx - 1, 1].item() # -1 as end_token_idx is exclusive for token slice\n",
    "        \n",
    "        chunk_texts.append(window_text[char_start:char_end])\n",
    "\n",
    "        # If the end of this chunk is the end of all actual tokens, we're done with this window\n",
    "        if end_token_idx == actual_num_tokens:\n",
    "            break\n",
    "            \n",
    "    if not chunk_embeddings_list: \n",
    "        return [], np.array([]) # No chunks were created\n",
    "    \n",
    "    # Stack all chunk embeddings and normalize them\n",
    "    chunk_embeddings_tensor = torch.stack(chunk_embeddings_list)\n",
    "    normalized_embeddings = F.normalize(chunk_embeddings_tensor, p=2, dim=1)\n",
    "    \n",
    "    return chunk_texts, normalized_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on the first few pages of the PDF to make sure it's correct. The early pages should fit comfortably inside a single model call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2994"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab the first few pages of the PDF to test a single late chunking\n",
    "sample_pages = ' '.join(clean_texts[:5])\n",
    "sample_toks = tokenizer(sample_pages); len(sample_toks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, we have plenty of room in a single model call to embed the first pages of the PDF. Let's go ahead and encode them. We'll group the document into fixed token windows of `256`, with an overlap of `128`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_num_tokens 2994\n"
     ]
    }
   ],
   "source": [
    "# fixed size of token groups for final embeddings\n",
    "token_sz = 256\n",
    "# overlap between the token groups\n",
    "token_overlap = 128\n",
    "\n",
    "# late chunk \n",
    "sample_chunks, sample_embeds = late_chunking(\n",
    "    sample_pages,\n",
    "    model, \n",
    "    tokenizer,\n",
    "    chunk_token_size=token_sz,\n",
    "    chunk_token_overlap=token_overlap,\n",
    "    model_processing_max_len=MODEL_MAX_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 23)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_chunks), len(sample_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this match the number of windows we expected? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking we got the right number of windows\n",
    "def num_windows(input_size: int, window: int, overlap: int) -> int:\n",
    "    \"Finds the number of windows with overlap to slide over an input.\"\n",
    "    return math.ceil((input_size - window) / overlap) + 1\n",
    "\n",
    "num_windows(len(sample_toks[0]), token_sz, token_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: we use `ceil()` in the code above because we want to include the last window, even if it doesn't max out our token_sz. We definitely don't want to drop data, there could be important information in that segment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our late chunking is up and running. Now we have to extend it to Long Late Chunking, to handle the entire PDF. This amounts to calling the function above as many times as needed given the model's max sequence length. For now, we won't worry about overlap although that could easily be integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_late_chunking(\n",
    "    document_text: str,\n",
    "    model: AutoModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    chunk_token_size: int,\n",
    "    chunk_token_overlap: int = 0,\n",
    "    processing_window_size: Optional[int] = None,\n",
    "    processing_window_overlap: int = 0\n",
    ") -> Tuple[List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Applies late chunking to a document using a sliding window approach.\n",
    "\n",
    "    This method is suitable for documents longer than the model's maximum context length.\n",
    "    The document is processed in overlapping windows. Each window's content is then\n",
    "    processed by `late_chunk_tokens_from_window` to generate smaller, fixed-size token chunks.\n",
    "\n",
    "    Args:\n",
    "        document_text: The full text of the document.\n",
    "        model: The pre-trained Hugging Face model, moved to the correct device.\n",
    "        tokenizer: The pre-trained Hugging Face tokenizer.\n",
    "        chunk_token_size: The desired size of each final chunk in tokens (e.g., 256, 512).\n",
    "        chunk_token_overlap: Overlap between final chunks within a window (default: 0).\n",
    "        processing_window_size: Maximum number of tokens for one model processing window.\n",
    "                                Defaults to tokenizer's model_max_length or MODEL_MAX_LENGTH.\n",
    "        processing_window_overlap: Overlap between processing windows (default: 0).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - A list of all chunk text strings from the document.\n",
    "            - A NumPy array of all chunk embeddings (L2 normalized). Returns empty if no\n",
    "              chunks are produced.\n",
    "    \"\"\"\n",
    "    if not document_text.strip():\n",
    "        return [], np.array([])\n",
    "\n",
    "    if processing_window_size is None:\n",
    "        processing_window_size = getattr(tokenizer, 'model_max_length', MODEL_MAX_LENGTH)\n",
    "    \n",
    "    # Validate overlaps are less than their respective sizes\n",
    "    if processing_window_overlap >= processing_window_size :\n",
    "        raise ValueError(f\"processing_window_overlap ({processing_window_overlap}) must be less than \"\n",
    "                         f\"processing_window_size ({processing_window_size}).\")\n",
    "    if chunk_token_overlap >= chunk_token_size :\n",
    "        raise ValueError(f\"chunk_token_overlap ({chunk_token_overlap}) must be less than \"\n",
    "                         f\"chunk_token_size ({chunk_token_size}).\")\n",
    "\n",
    "    # Tokenize the entire document once to get global token IDs and their character offsets.\n",
    "    # No truncation or padding here, as this is for defining the sliding windows.\n",
    "    full_inputs = tokenizer(\n",
    "        document_text,\n",
    "        return_tensors=None, # Get Python lists/integers for easier CPU-side manipulation\n",
    "        truncation=False, \n",
    "        padding=False, \n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    global_input_ids = full_inputs['input_ids'] # List of token IDs\n",
    "    global_offsets = full_inputs['offset_mapping'] # List of (start_char, end_char) tuples\n",
    "    \n",
    "    total_document_tokens = len(global_input_ids)\n",
    "    \n",
    "    if total_document_tokens == 0: \n",
    "        return [], np.array([])\n",
    "\n",
    "    # If document is shorter than or fits within one processing window, process directly\n",
    "    if total_document_tokens <= processing_window_size:\n",
    "        return late_chunking(\n",
    "            window_text=document_text, # The whole document is one window\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            chunk_token_size=chunk_token_size,\n",
    "            chunk_token_overlap=chunk_token_overlap,\n",
    "            model_processing_max_len=processing_window_size \n",
    "        )\n",
    "\n",
    "    all_collected_texts: List[str] = []\n",
    "    all_collected_embeddings_list: List[np.ndarray] = [] # List to store numpy arrays from each window\n",
    "    \n",
    "    window_stride = processing_window_size - processing_window_overlap\n",
    "    if window_stride <= 0:\n",
    "         raise ValueError(f\"processing_window_size ({processing_window_size}) must be greater than \"\n",
    "                          f\"processing_window_overlap ({processing_window_overlap}) to make progress.\")\n",
    "\n",
    "    # Iterate through the document tokens using a sliding window\n",
    "    for i in range(0, total_document_tokens, window_stride):\n",
    "        window_start_token_idx = i\n",
    "        window_end_token_idx = min(i + processing_window_size, total_document_tokens)\n",
    "        \n",
    "        # Extract the text for the current processing window using global character offsets\n",
    "        current_window_char_start = global_offsets[window_start_token_idx][0]\n",
    "        # The last token in the window is at index window_end_token_idx - 1\n",
    "        current_window_char_end = global_offsets[window_end_token_idx - 1][1]\n",
    "        \n",
    "        current_window_text = document_text[current_window_char_start:current_window_char_end]\n",
    "        \n",
    "        if not current_window_text.strip(): # Skip if the extracted window text is effectively empty\n",
    "            continue\n",
    "\n",
    "        # Process this window to get its late-chunked texts and embeddings\n",
    "        texts_from_window, embeddings_from_window = late_chunking(\n",
    "            window_text=current_window_text,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            chunk_token_size=chunk_token_size,\n",
    "            chunk_token_overlap=chunk_token_overlap,\n",
    "            model_processing_max_len=processing_window_size\n",
    "        )\n",
    "        \n",
    "        if texts_from_window: # If any chunks were produced for this window\n",
    "            all_collected_texts.extend(texts_from_window)\n",
    "            all_collected_embeddings_list.append(embeddings_from_window) # Append numpy array\n",
    "\n",
    "        # If the end of this window is the end of the document, we're done\n",
    "        if window_end_token_idx == total_document_tokens:\n",
    "            break\n",
    "            \n",
    "    if not all_collected_embeddings_list:\n",
    "        return [], np.array([]) # No embeddings were generated from any window\n",
    "        \n",
    "    # Concatenate all numpy arrays of embeddings into a single array\n",
    "    final_embeddings = np.concatenate(all_collected_embeddings_list, axis=0)\n",
    "    return all_collected_texts, final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Late Chunking in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run Late Chunking over the entire document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the entire PDF\n",
    "all_chunks, all_embeddings = long_late_chunking(\n",
    "    document_text=all_text,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_token_size=token_sz,\n",
    "    chunk_token_overlap=token_overlap,\n",
    "    processing_window_size=MODEL_MAX_LENGTH,\n",
    "    processing_window_overlap=0\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(all_chunks)} chunks.\")\n",
    "print(f\"Embeddings shape: {all_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding relevant section of the 10K filing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nomic uses mean pooling for their embeddings, as do many of the more powerful recent encoder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask a relevant question for the embeddings\n",
    "query = \"How much outstanding commercial paper did Apple have between September 2023 and 2024?\"\n",
    "\n",
    "# tokenize the query\n",
    "query_for_model = \"search_query: \" + query\n",
    "query_inputs = tokenizer(query_for_model, return_tensors='pt', truncation=True, max_length=tokenizer.model_max_length)\n",
    "with torch.no_grad():\n",
    "    query_outputs = model(**query_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"Canonical mean pooling of final token embeddings.\"\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )\n",
    "\n",
    "# get the mean-pooled, normalized query embedding\n",
    "query_embedding = mean_pooling(query_outputs, query_inputs['attention_mask'])\n",
    "query_embedding_normalized = F.normalize(query_embedding, p=2, dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How much outstanding commercial paper did Apple have between September of 2023 and 2024?\n",
      "Best matching chunk (idx 245, similarity: 0.6657):\n",
      " share repurchases. As of September 28, 2024 and September 30, 2023, the Company had $10.0 billion and $6.0 billion of commercial paper outstanding, respectively, with maturities generally less than nine months. The weighted-average interest rate of the Company’s commercial paper was 5.00% and 5.28% as of September 28, 2024 and September 30, 2023, respectively. The following table provides a summary of cash flows associated with the issuance and maturities of commercial paper for 2024, 2023 and 2022 (in millions): 2024 2023 2022 Maturities 90 days or less: Proceeds from/(Repayments of) commercial paper, net $ 3,960 $ (1,333) $ 5,264 Maturities greater than 90 days: Proceeds from commercial paper — — 5,948 Repayments of commercial paper — (2,645) (7,257) Proceeds from/(Repayments of) commercial paper, net — (2,645) (1,309) Total proceeds from/(repayments of) commercial paper, net $ 3,960 $ (3,978) $ 3,955 Term Debt The Company has\n"
     ]
    }
   ],
   "source": [
    "# look for similar matches\n",
    "similarities = np.dot(all_embeddings, query_embedding_normalized.T).squeeze()\n",
    "best_match_idx = np.argmax(similarities)\n",
    "\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"Best matching chunk (idx {best_match_idx}, similarity: {similarities[best_match_idx]:.4f}):\")\n",
    "print(all_chunks[best_match_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "late-chunking",
   "language": "python",
   "name": "late-chunking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
