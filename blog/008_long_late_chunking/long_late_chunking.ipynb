{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Late Chunking on long documents\n",
    "author: Chris Kroenke\n",
    "draft: false\n",
    "date: '2025-04-10'\n",
    "date-modified: '2025-04-10'\n",
    "image: LateChunking_title_image.png\n",
    "toc: true\n",
    "description: Late-chunking might be the closest we've ever been to solving retrieval with text embeddings models. This post gets it working on very long documents.\n",
    "tags:\n",
    "  - embeddings\n",
    "  - retrieval\n",
    "  - RAG\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "    grid:\n",
    "      body-width: 1200px\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-word !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Late-chunking might be the closest we've ever been to solving retrieval with text embeddings models. I've been testing it other the weeks, and it's the first time where it feels like I barely need any LLM parsing and formatting of the retrieved information. It's so good, that the passages of text that end up being retrieved basically directly answer my question ."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
