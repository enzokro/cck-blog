{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: LLMs as Complex Sequence Learners\n",
    "author: Chris Kroenke\n",
    "draft: false\n",
    "date: '2025-04-19'\n",
    "date-modified: '2025-04-19'\n",
    "image: logo.png\n",
    "toc: true\n",
    "description: A look at how LLMs learn, and what this says about their limitations.\n",
    "tags:\n",
    "  - embeddings\n",
    "  - retrieval\n",
    "  - RAG\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "    grid:\n",
    "      body-width: 1200px\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-word !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This is a rough exploration and hypothesis of how LLMs learn, and why they have become so powerful.\n",
    "\n",
    "There is a typical attack or dismissal that says LLMs are \"simply predicting the next word.\" This was maybe true of the very earliest pre-training efforts. But the way models are trained today is a far cry, in both data quality and scale, from those early dats. In my opinions, that is not meant as an earnest critique of LLMs. It is a motivated stab at the models driven by x, y, or z biased reason. \n",
    "\n",
    "LLMs are not predicting the next word, they're predicting a ton of masked-out, blank words, chosen at randomly. Imagine you were listening to a conversation, and the other speaker randomly decided to skip 1/3rd of all given words in a sentence. Unless you knew the person or context very well, you'd likely struggle to fully keep up with the conversation. Not to mention how frustrating it would be. This masked fill-in is what LLMs learn to do during pre-training over massive databases that not even the fastest human reader on earth could get through in several lifetimes.\n",
    "\n",
    "In our conversation example, you'd also be doing this word fill-in with the loose 7 +/- 2 objects that we humans hold in working memory. LLMs are doing this with working memories (context windows) that have becomes thousands of words long. Imagine you're reading a big, a big book, one of those massive old Russian novels that drag out the reality of the human condition, cold and kicking and screaming, into the light and makes it bare for all to see. Imagine you're on page 400, and all of the sudden a 1/3 of the words on the next page are missing. When you to to fill them, imagine you'd have the entire book so far, word for words, memorized and could use all of it to fill in the gap. This would clearly be a super-human thing to do, but you also wouldn't realistically need *all* the book to accurately predict the next page. \n",
    "\n",
    "The examples of the dropped-conversation and the book-completion stretch the imagination, but are meant to drive home both how dissimilar and more powerful modern LLM pre-training is from \"just predicting the next word,\" especially with the heavy human bias we attach to that pesky \"just.\"\n",
    "\n",
    "The above only covers pre-training. There is also the art of post-training, or SFT and RL, that has evolved to a complete and complex science on its own. What the models are taught to do during these stages go far, far beyond predicting the next linear word. \n",
    "\n",
    "During pre-training, the model an implicit mapping and dynamic flow of human language, syntax, and communication. All they see are long sequences of tokens, and they're able to attend over these tokens and see how they're related to each other. From these sequences and patterns, they're able to learn what sequences make \"sense\" in the landscape of all possible configuration of words, letters, and punctuation.   of syntaxI would like to argument in this blog post that post-training "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
