% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

<style>
.cell-output-stdout code {
  word-break: break-word !important;
  white-space: pre-wrap !important;
}
</style>
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Lesson 3: HuggingFace NLP Models},
  pdfauthor={Chris Kroenke},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Lesson 3: HuggingFace NLP Models}
\author{Chris Kroenke}
\date{2023-10-07}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\begin{quote}
Running powerful NLP models with the HuggingFace \texttt{transformers}
library.
\end{quote}

\section{Intro}\label{intro}

Welcome to the third lesson of the course. Let's recap our progress so
far:

\begin{itemize}
\tightlist
\item
  Lesson 1: We made a python environment for LLMs.\\
\item
  Lesson 2: Set up a personal blog to track our progress.
\end{itemize}

Next we will use our first LLM. We'll start with a Natural Language
Processing (NLP) model provided by the HuggingFace team.

\subsection{Notebook best practices}\label{notebook-best-practices}

First, let's set up our notebook to be fully interactive and easy to
use. We can do this with a couple of ``magic functions'' built-in to
Jupyter.

Specifically, we use the magic \texttt{autoreload} and
\texttt{matplotlib} functions. The cell below shows them in action:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# best practice notebook magic}
\OperatorTok{\%}\NormalTok{load\_ext autoreload}
\OperatorTok{\%}\NormalTok{autoreload }\DecValTok{2}
\OperatorTok{\%}\NormalTok{matplotlib inline}
\end{Highlighting}
\end{Shaded}

Let's take a look at what these magic functions do.

\texttt{autoreload} dynamically reloads code libraries, even as they're
changing under the hood. That means we do not have to restart the
notebook after every change. We can instead code and experiment on the
fly.

\texttt{matplotlib\ inline} automatically displays any plots below the
code cell that created them. The plots are also saved in the notebook
itself, which is perfect for our blog posts.

All of our notebooks going forward will start with these magic
functions.

Let's start with the \texttt{"hello,\ world!"} of NLP: sentiment
analysis.

\section{Sentiment Analysis with
HuggingFace}\label{sentiment-analysis-with-huggingface}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, left=2mm, opacitybacktitle=0.6, toprule=.15mm, coltitle=black, opacityback=0, arc=.35mm, colframe=quarto-callout-note-color-frame, breakable, bottomtitle=1mm, bottomrule=.15mm, colback=white, rightrule=.15mm, titlerule=0mm, toptitle=1mm]

The code and examples below are based on the official HuggingFace
tutorial, reworked to better suit the course.

\end{tcolorbox}

Imagine that we're selling some product. And we've gathered a bunch of
reviews from a large group of users to find out both the good and bad
things that people are saying. The bad reviews will point out where our
product needs improving. Positive reviews will show what we're doing
right.

Figuring out the tone of a statement (\emph{positive vs.~negative}) is
an area of NLP known as \texttt{sentiment\ analysis}.

Going through each review would give us a ton of insight about our
product. But, it would take a ton of intense and manual effort. Enter
Machine Learning to the rescue! An NLP model can automatically analyze
and classify the reviews in bulk.

\subsection{First, a Pipeline}\label{first-a-pipeline}

Let's take a look at the HuggingFace NLP model that we'll run. At a high
level, the model is built around three key pieces:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A \texttt{Config} file.\\
\item
  A \texttt{Preprocessor} file.\\
\item
  \texttt{Model} file(s).
\end{enumerate}

The HuggingFace API has a handy, high-level \texttt{pipeline} that wraps
up all three objects for us.

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, left=2mm, opacitybacktitle=0.6, toprule=.15mm, coltitle=black, opacityback=0, arc=.35mm, colframe=quarto-callout-important-color-frame, breakable, bottomtitle=1mm, bottomrule=.15mm, colback=white, rightrule=.15mm, titlerule=0mm, toptitle=1mm]

Before going forward, make sure that the \texttt{llm-env} environment
from the first lesson is active. This environment has the HuggingFace
libraries used below.

\end{tcolorbox}

The code below uses the \texttt{transformers} library to build a
Sentiment Analysis \texttt{pipeline}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load in the pipeline object from HuggingFace}
\ImportTok{from}\NormalTok{ transformers }\ImportTok{import}\NormalTok{ pipeline}

\CommentTok{\# create a sentiment analysis pipeline}
\NormalTok{classifier }\OperatorTok{=}\NormalTok{ pipeline(}\StringTok{"sentiment{-}analysis"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
\end{verbatim}

Since we didn't specify a model, you can see in the output above that
HuggingFace picked a
\href{distilbert-base-uncased-finetuned-sst-2-english}{distilbert model}
for us by default.

We will learn more about what exactly \texttt{distilbert} is and how it
works later on. For now, think of it as a useful NLP genie who can look
at a sentence and tell us whether its has a positive or negative tone.

Next, let's find out what the model thinks about the sentence:
\texttt{"HuggingFace\ pipelines\ are\ awesome!"}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# sentiment analysis on a simple, example sentence}
\NormalTok{example\_sentence }\OperatorTok{=} \StringTok{"HuggingFace pipelines are awesome!"}
\NormalTok{classifier(example\_sentence)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[{'label': 'POSITIVE', 'score': 0.9998503923416138}]
\end{verbatim}

Not bad. We see a strong confident score for a \texttt{POSITIVE} label,
as could be expected.

We can also pass many sentences at once, which starts to show the bulk
processing power of these models. Let's process four sentences at once:
three positive ones, and a clearly negative one.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# many sentences at once, in a python list}
\NormalTok{many\_sentences }\OperatorTok{=}\NormalTok{ [}
    \StringTok{"HuggingFace pipelines are awesome!"}\NormalTok{,}
    \StringTok{"I hope you\textquotesingle{}re enjoying this course so far"}\NormalTok{,}
    \StringTok{"Hopefully the material is clear and useful"}\NormalTok{,}
    \StringTok{"I don\textquotesingle{}t like this course so far"}\NormalTok{,}
\NormalTok{]}

\CommentTok{\# process many sentences at once}
\NormalTok{results }\OperatorTok{=}\NormalTok{ classifier(many\_sentences)}

\CommentTok{\# check the tone of each sentence}
\ControlFlowTok{for}\NormalTok{ result }\KeywordTok{in}\NormalTok{ results:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"label: }\SpecialCharTok{\{}\NormalTok{result[}\StringTok{\textquotesingle{}label\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{, with score: }\SpecialCharTok{\{}\BuiltInTok{round}\NormalTok{(result[}\StringTok{\textquotesingle{}score\textquotesingle{}}\NormalTok{], }\DecValTok{4}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
label: POSITIVE, with score: 0.9999
label: POSITIVE, with score: 0.9998
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.8758
\end{verbatim}

Congrats! You've now ran a HuggingFace pipeline and used it to analyze
the tone of a few sentences. Next, let's take a closer look at the
pipeline object.

\section{\texorpdfstring{Going inside the
\texttt{pipeline}}{Going inside the pipeline}}\label{going-inside-the-pipeline}

Under the hood, a pipeline handles three key HuggingFace NLP pieces:
Config, Preprocessor, and Model.

To better understand each piece, let's take one small step down the
ladder of abstraction and build our own simple pipeline.

We will use the same \texttt{distilbert} model from before. First we
need the three key pieces mentioned above. Thankfully, we can import
each of these pieces from the \texttt{transformers} library.

\subsection{Config class}\label{config-class}

The \texttt{config} class is a simple map with the options and
configurations of a model. It has the key-value pairs that define a
model's architecture and hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# config for the model}
\ImportTok{from}\NormalTok{ transformers }\ImportTok{import}\NormalTok{ DistilBertConfig}
\end{Highlighting}
\end{Shaded}

\subsection{Preprocessor class}\label{preprocessor-class}

The \texttt{preprocessor} object in this case is a \texttt{Tokenizer}.
Tokenizers convert strings and characters into special tensor inputs for
the LLM.

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, left=2mm, opacitybacktitle=0.6, toprule=.15mm, coltitle=black, opacityback=0, arc=.35mm, colframe=quarto-callout-note-color-frame, breakable, bottomtitle=1mm, bottomrule=.15mm, colback=white, rightrule=.15mm, titlerule=0mm, toptitle=1mm]

Correctly pre-processing inputs is one of the most important and
error-prone steps in using ML models. In other words, it's good to
offload to a class that's already been tested and debugged.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# input preprocessor to tokenize strings}
\ImportTok{from}\NormalTok{ transformers }\ImportTok{import}\NormalTok{ DistilBertTokenizer}
\end{Highlighting}
\end{Shaded}

\subsection{Model class}\label{model-class}

The \texttt{model} class holds the weights and parameters for the actual
LLM. It's the ``meat and bones'' of the setup, so to speak.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the text classifier model}
\ImportTok{from}\NormalTok{ transformers }\ImportTok{import}\NormalTok{ DistilBertForSequenceClassification}
\end{Highlighting}
\end{Shaded}

\subsection{Naming the model}\label{naming-the-model}

We need to know a model's full, proper name in to load it from
HuggingFace. Its name is how we find the model on the
\href{https://huggingface.co/docs/hub/models-the-hub}{HuggingFace Model
Hub}.

Once we know its full name, there is a handy \texttt{from\_pretrained()}
function that will automatically find and download the pieces for us.

In this case, the distilbert model's full name is:\\
\textgreater{} \texttt{distilbert-base-uncased-finetuned-sst-2-english}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# sentiment analysis model name}
\NormalTok{model\_name }\OperatorTok{=} \StringTok{\textquotesingle{}distilbert{-}base{-}uncased{-}finetuned{-}sst{-}2{-}english\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

In the code below we can now load each of the three NLP pieces for this
model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create the config}
\NormalTok{config }\OperatorTok{=}\NormalTok{ DistilBertConfig.from\_pretrained(model\_name)}

\CommentTok{\# create the input tokenizer }
\NormalTok{tokenizer }\OperatorTok{=}\NormalTok{ DistilBertTokenizer.from\_pretrained(model\_name)}

\CommentTok{\# create the model}
\NormalTok{model }\OperatorTok{=}\NormalTok{ DistilBertForSequenceClassification.from\_pretrained(model\_name)}
\end{Highlighting}
\end{Shaded}

Next we will compose these three pieces together to mimic the original
\texttt{pipeline} example.

\subsection{\texorpdfstring{Putting together a
\texttt{simple\_pipeline}}{Putting together a simple\_pipeline}}\label{putting-together-a-simple_pipeline}

\subsubsection{Preprocessing the inputs}\label{preprocessing-the-inputs}

First, we create a \texttt{preprocess} function to turn a given
\texttt{text} string into the proper, tokenized inputs than an LLM
expects.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ preprocess(text: }\BuiltInTok{str}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Sends \textasciigrave{}text\textasciigrave{} through the model\textquotesingle{}s tokenizer.  }
\CommentTok{    The tokenizer turns words and characters into proper inputs for an NLP model.}
\CommentTok{    """}
\NormalTok{    tokenized\_inputs }\OperatorTok{=}\NormalTok{ tokenizer(text, return\_tensors}\OperatorTok{=}\StringTok{\textquotesingle{}pt\textquotesingle{}}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ tokenized\_inputs}
\end{Highlighting}
\end{Shaded}

Let's test this preprocessing function on the example sentence from
earlier.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# manually preprocessing the example sentence: "HuggingFace pipelines are awesome!"}
\NormalTok{preprocess(example\_sentence)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'input_ids': tensor([[  101, 17662, 12172, 13117,  2015,  2024, 12476,   999,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}
\end{verbatim}

It turned an input string into numerical embeddings for the LLM. We'll
breakdown what exactly this output means later on in the course. For
now, think of it as sanitizing and formatting the text into a format
that the LLM has been trained to work with.

\subsubsection{Runnning the model}\label{runnning-the-model}

Next up, let's make our own \texttt{forward} function that run the LLM
on preprocessed inputs.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ forward(text: }\BuiltInTok{str}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    First we preprocess the \textasciigrave{}text\textasciigrave{} into tokens.}
\CommentTok{    Then we send the \textasciigrave{}tokenized\_inputs\textasciigrave{} to the model.}
\CommentTok{    """}
\NormalTok{    tokenized\_inputs }\OperatorTok{=}\NormalTok{ preprocess(text)}
\NormalTok{    outputs }\OperatorTok{=}\NormalTok{ model(}\OperatorTok{**}\NormalTok{tokenized\_inputs)}
    \ControlFlowTok{return}\NormalTok{ outputs}
\end{Highlighting}
\end{Shaded}

Let's check what this outputs for our running example sentence.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{outputs }\OperatorTok{=}\NormalTok{ forward(example\_sentence)}\OperatorTok{;}\NormalTok{ outputs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
SequenceClassifierOutput(loss=None, logits=tensor([[-4.2326,  4.5748]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
\end{verbatim}

You'll see a lot going on in the \texttt{SequenceClassifierOutput}
above. To be honest, this is where the original \texttt{pipeline} does
most of the heavy-lifting for us. It takes the raw, detailed output from
an LLM and converts it into a more human-readable format.

We'll mimic this heavy-lifting by using the \texttt{Config} class and
model outputs to find out whether the sentence is positive or negative.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ process\_outputs(outs):}
    \CommentTok{"""}
\CommentTok{    Converting the raw model outputs into a human{-}readable result.}

\CommentTok{    Steps:}
\CommentTok{        1. Grab the raw "scores" from the model for Positive and Negative labels.  }
\CommentTok{        2. Find out which score is the highest (aka the model\textquotesingle{}s decision).  }
\CommentTok{        3. Use the \textasciigrave{}config\textasciigrave{} object to find the class label for the highest score.  }
\CommentTok{        4. Turn the raw score into a human{-}readable probability value.  }
\CommentTok{        5. Print out the predicted labels with its probability.  }
\CommentTok{    """}
    \CommentTok{\# 1. Grab the raw "scores" that from the model for Positive and Negative labels}
\NormalTok{    logits }\OperatorTok{=}\NormalTok{ outs.logits}

    \CommentTok{\# 2. Find the strongest label score, aka the model\textquotesingle{}s decision}
\NormalTok{    pred\_idx }\OperatorTok{=}\NormalTok{ logits.argmax(}\DecValTok{1}\NormalTok{).item()}

    \CommentTok{\# 3. Use the \textasciigrave{}config\textasciigrave{} object to find the class label}
\NormalTok{    pred\_label }\OperatorTok{=}\NormalTok{ config.id2label[pred\_idx]  }

    \CommentTok{\# 4. Calculate the human{-}readable number for the score}
\NormalTok{    pred\_score }\OperatorTok{=}\NormalTok{ logits.softmax(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)[:, pred\_idx].item()}

    \CommentTok{\# 5. return the label and score in a dictionary}
    \ControlFlowTok{return}\NormalTok{ \{}
        \StringTok{\textquotesingle{}label\textquotesingle{}}\NormalTok{: pred\_label,}
        \StringTok{\textquotesingle{}score\textquotesingle{}}\NormalTok{: pred\_score, }
\NormalTok{    \}}
\end{Highlighting}
\end{Shaded}

We can now put together a \texttt{simple\_pipeline}, and check how it
compares to the original \texttt{pipeline}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ simple\_pipeline(text):}
    \CommentTok{"""}
\CommentTok{    Putting the NLP pieces and functions together into a pipeline.}
\CommentTok{    """}
    \CommentTok{\# get the model\textquotesingle{}s raw output}
\NormalTok{    model\_outs }\OperatorTok{=}\NormalTok{ forward(text)}
    \CommentTok{\# convert the raw outputs into a human readable result}
\NormalTok{    predictions }\OperatorTok{=}\NormalTok{ process\_outputs(model\_outs)}
    \ControlFlowTok{return}\NormalTok{ predictions}
\end{Highlighting}
\end{Shaded}

Calling the \texttt{simple\_pipeline} on the example sentence, drumroll
please\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# running our simple pipeline on the example text}
\NormalTok{simple\_pipeline(example\_sentence)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'label': 'POSITIVE', 'score': 0.9998503923416138}
\end{verbatim}

And just like that, we too a small peek under the \texttt{pipeline} hood
and built our own, simple working version.

One pain point: we had to know the full, proper name of the different
\texttt{Distilbert*} pieces to import the Config, Preprocessor, and
Model. This gets overwhelming fast given the flood of LLM models
released almost daily. Thankfully, HuggingFace has come up with a great
solution to this problem: the \texttt{Auto} class.

\section{\texorpdfstring{True HuggingFace magic: \texttt{Auto}
classes}{True HuggingFace magic: Auto classes}}\label{true-huggingface-magic-auto-classes}

With \texttt{Auto} classes, we don't have to know the exact or proper
name of the LLM's objects to import them. We only need the proper name
of the model on the hub:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# viewing our distilbert model\textquotesingle{}s name}
\NormalTok{model\_name}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'distilbert-base-uncased-finetuned-sst-2-english'
\end{verbatim}

Run the cell below to import the Auto classes. Then we'll use them with
the model name to create an even cleaner \texttt{simple\_pipeline}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# importing the Auto classes}
\ImportTok{from}\NormalTok{ transformers }\ImportTok{import}\NormalTok{ AutoConfig}
\ImportTok{from}\NormalTok{ transformers }\ImportTok{import}\NormalTok{ AutoTokenizer}
\ImportTok{from}\NormalTok{ transformers }\ImportTok{import}\NormalTok{ AutoModelForSequenceClassification}
\end{Highlighting}
\end{Shaded}

Next we create the three key NLP pieces with the Auto classes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# building the pieces with \textasciigrave{}Auto\textasciigrave{} classes}
\NormalTok{config }\OperatorTok{=}\NormalTok{ AutoConfig.from\_pretrained(model\_name)}
\NormalTok{tokenizer }\OperatorTok{=}\NormalTok{ AutoTokenizer.from\_pretrained(model\_name)}
\NormalTok{model }\OperatorTok{=}\NormalTok{ AutoModelForSequenceClassification.from\_pretrained(model\_name)}
\end{Highlighting}
\end{Shaded}

We can now use these pieces to build a \texttt{simple\_pipeline} class
that's cleaner than before, and can handle any model\_name:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ SentimentPipeline:}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, model\_name: }\BuiltInTok{str}\NormalTok{):}
        \CommentTok{"""}
\CommentTok{        Simple Sentiment Analysis pipeline.}
\CommentTok{        """}
        \VariableTok{self}\NormalTok{.model\_name }\OperatorTok{=}\NormalTok{ model\_name}
        \VariableTok{self}\NormalTok{.config }\OperatorTok{=}\NormalTok{ AutoConfig.from\_pretrained(}\VariableTok{self}\NormalTok{.model\_name)}
        \VariableTok{self}\NormalTok{.tokenizer }\OperatorTok{=}\NormalTok{ AutoTokenizer.from\_pretrained(}\VariableTok{self}\NormalTok{.model\_name)}
        \VariableTok{self}\NormalTok{.model }\OperatorTok{=}\NormalTok{ AutoModelForSequenceClassification.from\_pretrained(}\VariableTok{self}\NormalTok{.model\_name)}

    \KeywordTok{def}\NormalTok{ preprocess(}\VariableTok{self}\NormalTok{, text: }\BuiltInTok{str}\NormalTok{):}
        \CommentTok{"""}
\CommentTok{        Sends \textasciigrave{}text\textasciigrave{} through the LLM\textquotesingle{}s tokenizer.  }
\CommentTok{        The tokenizer turns words and characters into special inputs for the LLM.}
\CommentTok{        """}
\NormalTok{        tokenized\_inputs }\OperatorTok{=} \VariableTok{self}\NormalTok{.tokenizer(text, return\_tensors}\OperatorTok{=}\StringTok{\textquotesingle{}pt\textquotesingle{}}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ tokenized\_inputs}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, text: }\BuiltInTok{str}\NormalTok{):}
        \CommentTok{"""}
\CommentTok{        First we preprocess the \textasciigrave{}text\textasciigrave{} into tokens.}
\CommentTok{        Then we send the \textasciigrave{}token\_inputs\textasciigrave{} to the model.}
\CommentTok{        """}
\NormalTok{        token\_inputs }\OperatorTok{=} \VariableTok{self}\NormalTok{.preprocess(text)}
\NormalTok{        outputs }\OperatorTok{=} \VariableTok{self}\NormalTok{.model(}\OperatorTok{**}\NormalTok{token\_inputs)}
        \ControlFlowTok{return}\NormalTok{ outputs}

    \KeywordTok{def}\NormalTok{ process\_outputs(}\VariableTok{self}\NormalTok{, outs):}
        \CommentTok{"""}
\CommentTok{        Here we mimic the post{-}processing that HuggingFace automatically does in its \textasciigrave{}pipeline\textasciigrave{}.  }
\CommentTok{        """}
        \CommentTok{\# grab the raw scores from the model for Positive and Negative labels}
\NormalTok{        logits }\OperatorTok{=}\NormalTok{ outs.logits}

        \CommentTok{\# find the strongest label score, aka the model\textquotesingle{}s decision}
\NormalTok{        pred\_idx }\OperatorTok{=}\NormalTok{ logits.argmax(}\DecValTok{1}\NormalTok{).item()}

        \CommentTok{\# use the \textasciigrave{}config\textasciigrave{} object to find the actual class label}
\NormalTok{        pred\_label }\OperatorTok{=} \VariableTok{self}\NormalTok{.config.id2label[pred\_idx]  }

        \CommentTok{\# calculate the human{-}readable probability score for this class}
\NormalTok{        pred\_score }\OperatorTok{=}\NormalTok{ logits.softmax(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)[:, pred\_idx].item()}

        \CommentTok{\# return the predicted label and its score}
        \ControlFlowTok{return}\NormalTok{ \{}
            \StringTok{\textquotesingle{}label\textquotesingle{}}\NormalTok{: pred\_label,}
            \StringTok{\textquotesingle{}score\textquotesingle{}}\NormalTok{: pred\_score, }
\NormalTok{        \}}
    
    \KeywordTok{def} \FunctionTok{\_\_call\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, text: }\BuiltInTok{str}\NormalTok{):}
        \CommentTok{"""}
\CommentTok{        Overriding the call method to easily and intuitively call the pipeline.}
\CommentTok{        """}
\NormalTok{        model\_outs }\OperatorTok{=} \VariableTok{self}\NormalTok{.forward(text)}
\NormalTok{        preds }\OperatorTok{=} \VariableTok{self}\NormalTok{.process\_outputs(model\_outs)}
        \ControlFlowTok{return}\NormalTok{ preds}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Using the custom
\texttt{SentimentPipeline}}{Using the custom SentimentPipeline}}\label{using-the-custom-sentimentpipeline}

Let's leverage both the new class and a different model, to show the
power of Auto classes.

For fun, let's use BERT model that was trained specifically on tweets.
The full model's name is
\href{https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis}{\texttt{finiteautomata/bertweet-base-sentiment-analysis}}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# using a different model}
\NormalTok{new\_model\_name }\OperatorTok{=} \StringTok{\textquotesingle{}finiteautomata/bertweet{-}base{-}sentiment{-}analysis\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating a new sentiment pipeline}
\NormalTok{simple\_pipeline }\OperatorTok{=}\NormalTok{ SentimentPipeline(new\_model\_name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0
\end{verbatim}

Now let's run it on our handy example sentence.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calling our new, flexible pipeline}
\NormalTok{simple\_pipeline(example\_sentence)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'label': 'POS', 'score': 0.9908382296562195}
\end{verbatim}

Congrats! You've now built a flexible pipeline for Sentiment Analysis
that can leverage most NLP models on the HuggingFace hub.

\section{Conlusion}\label{conlusion}

This notebook went through the basics of using a HuggingFace pipeline to
run sentiment analysis on a few sentences. We then looked under the hood
at the pipeline's three key pieces: Config, Preprocessor, and Model.

Lastly, we built our own \texttt{simple\_pipeline} from scratch to see
how the pieces fit together.

The goal of this notebook was two fold. First, we wanted to gain
hands-on experience with using the \texttt{transformers} API from
HuggingFace. It's an incredibly powerful library, that lets us do what
used to be difficult, research-level NLP tasks in a few lines of code.

Second, we wanted to get some familiarity with downloading models. The
model weights that we downloaded from HuggingFace are the same ones that
we will be fine-tuning, quantizing, and deploying on our devices
throughout the course.

There are two appendixes below. The first one gives a handy way of
counting the number of weights in a model. The second one goes into more
details about how to interactively debug an analyze the code in a
Jupyter notebook.

\section{Appendix 1: Counting the number of parameters in a
model}\label{appendix-1-counting-the-number-of-parameters-in-a-model}

The following code snippet counts the number of trainable parameters in
a model. It's a question that comes up often when working with LLMs, and
having a quick reference to find out a rough model's size often comes in
handy.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ count\_parameters(model):}
    \CommentTok{"""}
\CommentTok{    Counts the number of trainable parameters in a \textasciigrave{}model\textasciigrave{}.}
\CommentTok{    """}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(p.numel() }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ model.parameters() }\ControlFlowTok{if}\NormalTok{ p.requires\_grad)}
\end{Highlighting}
\end{Shaded}

Here we use it to count the number of parameters in the distilbert model
from above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# view the number of parameters in the last model used}
\SpecialStringTok{f"Number of trainable params: }\SpecialCharTok{\{}\NormalTok{count\_parameters(model)}\SpecialCharTok{:,\}}\SpecialStringTok{"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'Number of trainable params: 66,955,010'
\end{verbatim}

\section{\texorpdfstring{Appendix 2: Inspecting the \texttt{classifier},
notebook
style.}{Appendix 2: Inspecting the classifier, notebook style.}}\label{appendix-2-inspecting-the-classifier-notebook-style.}

What is the \texttt{classifier} object, exactly? Jupyter has many
powerful ways of inspecting and analyzing its code.

One of the simplest ways of checking an object is to call it by itself
in a code cell, as shown below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# show the contents of the \textasciigrave{}classifier\textasciigrave{} object}
\NormalTok{classifier}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<transformers.pipelines.text_classification.TextClassificationPipeline at 0x176de7850>
\end{verbatim}

We can see the \texttt{classifier} is a type of
\texttt{TextClassification} pipeline. This makes sense: we fed it an
input sentence and asked it to classify the statement as \emph{positive}
vs.~\emph{negative}.

There is also a tab-autocomplete feature to find the members and methods
of an object. For example, to look up everything in \texttt{classifier},
hit tab after adding a \texttt{.}.

Uncomment the cells below and hit the tab key to test the auto-complete
feature.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# tab after the \textasciigrave{}.\textasciigrave{} to auto{-}complete all variables/methods}
\CommentTok{\# classifier.}
\end{Highlighting}
\end{Shaded}

Let's say you vaguely remember the name of a variable or function, say
for example the \texttt{forward()} method. In that case you can type the
first few letters and hit tab to auto-complete the full set of options:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# tab after the \textasciigrave{}.for\textasciigrave{} to auto{-}complete the rest of the options}
\CommentTok{\# classifier.for}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Asking questions: \texttt{?} and
\texttt{??}}{Asking questions: ? and ??}}\label{asking-questions-and}

Lastly, we can literally interrogate an object in Jupyter for more
information.

If we tag a single \texttt{?} after an object, we'll get its basic
documentation (docstring). Note that we omit it here to keep the
notebook from getting too busy.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# the power of asking questions}
\NormalTok{classifier?}
\end{Highlighting}
\end{Shaded}

If we tag on \emph{two} question marks: \texttt{??}, then we get the
full source code of the object:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# really curious about classifier}
\NormalTok{classifier??}
\end{Highlighting}
\end{Shaded}

Both \texttt{?} and \texttt{??} are excellent and quick ways to look
under the hood of any object in Jupyter.

\subsection{\texorpdfstring{Inspecting a specific \texttt{classifier}
function}{Inspecting a specific classifier function}}\label{inspecting-a-specific-classifier-function}

Let's take a look at the function that does the heavy lifting for our
sentiment analysis task: \texttt{forward()}.

\begin{Shaded}
\begin{Highlighting}[]
 \CommentTok{\# looking at what actually runs the inputs}
\NormalTok{classifier.forward}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<bound method Pipeline.forward of <transformers.pipelines.text_classification.TextClassificationPipeline object at 0x176de7850>>
\end{verbatim}

What does this function actually do? Let's find out.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# source code of the forward function}
\NormalTok{classifier.forward??}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Signature: classifier.forward(model_inputs, **forward_params)
Docstring: <no docstring>
Source:   
    def forward(self, model_inputs, **forward_params):
        with self.device_placement():
            if self.framework == "tf":
                model_inputs["training"] = False
                model_outputs = self._forward(model_inputs, **forward_params)
            elif self.framework == "pt":
                inference_context = self.get_inference_context()
                with inference_context():
                    model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
                    model_outputs = self._forward(model_inputs, **forward_params)
                    model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device("cpu"))
            else:
                raise ValueError(f"Framework {self.framework} is not supported")
        return model_outputs
File:      ~/mambaforge/envs/llm_base/lib/python3.11/site-packages/transformers/pipelines/base.py
Type:      method
\end{verbatim}

We can see that it automatically handles whether we're running a
TensorFlow (\texttt{tf}) or PyTorch (\texttt{pt}) model. Then, it makes
sure the tensors are on the correct device. Lastly is calls another
function, \texttt{\_forward()} on the prepared inputs.

We can follow the rabbit hole as far down as needed. Let's take a look
at the source of \texttt{\_forward}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# going deeper}
\NormalTok{classifier.\_forward??}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Signature: classifier._forward(model_inputs)
Docstring:
_forward will receive the prepared dictionary from `preprocess` and run it on the model. This method might
involve the GPU or the CPU and should be agnostic to it. Isolating this function is the reason for `preprocess`
and `postprocess` to exist, so that the hot path, this method generally can run as fast as possible.

It is not meant to be called directly, `forward` is preferred. It is basically the same but contains additional
code surrounding `_forward` making sure tensors and models are on the same device, disabling the training part
of the code (leading to faster inference).
Source:   
    def _forward(self, model_inputs):
        # `XXXForSequenceClassification` models should not use `use_cache=True` even if it's supported
        model_forward = self.model.forward if self.framework == "pt" else self.model.call
        if "use_cache" in inspect.signature(model_forward).parameters.keys():
            model_inputs["use_cache"] = False
        return self.model(**model_inputs)
File:      ~/mambaforge/envs/llm_base/lib/python3.11/site-packages/transformers/pipelines/text_classification.py
Type:      method
\end{verbatim}

Ah, we can see it calls the \texttt{model} of the classifier. This is
the \texttt{distilbert} model we saw earlier! Now we can peek under the
hood at the actual Transformer LLM.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the distilbert sentiment analysis model}
\NormalTok{classifier.model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
DistilBertForSequenceClassification(
  (distilbert): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0-5): 6 x TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
\end{verbatim}

We will breakdown the different pieces in this model later on in the
course.

The important takeaway for now is that this shows the main structure of
most Transformer LLM models. The changes are mostly incremental from
this foundation.



\end{document}
